---
title: "Data exploration assignment"
author: "V. Komarova"
date: "2022-08-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
## knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```


```{r echo = TRUE, error = FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Loading packages
library(tidyverse)
library(purrr)
library(lubridate)
library(fixest)

```

```{r echo = TRUE, error = FALSE, message=FALSE, warning=FALSE}

# Preparing Google trends data

# Merging Google trends data
g_files <- list.files(path="../data", pattern = 'trends_up_to', full.names = TRUE)
G_data <- map_df(g_files, read_csv)

G_data <- na.omit(G_data)

#Converting date from string to date format

G_data <- G_data %>% 
  mutate(YMD = ymd(str_sub(G_data$monthorweek, end = 10)))

# creating a month variable for aggregation
G_data <- G_data %>% mutate(month = floor_date(YMD, unit = 'month'))

# Standardizing Index variable
G_data <- G_data %>% 
  group_by(schname) %>% 
  mutate(st_index = (index - mean(index))/sd(index))


```

```{r echo = TRUE, error = FALSE, message=FALSE, warning=FALSE}

#reading in the Scorecard Data
scorecard <- read_csv('../data/Most+Recent+Cohorts+(Scorecard+Elements).csv')
scorecard <- na.omit(scorecard)
name_links <- read_csv('../data/id_name_link.csv')

# removing duplicates
name_links <- name_links %>% 
  group_by(schname) %>% 
  mutate(n = n()) %>% 
  filter(n == 1)

# merging the data frames
g_trends_to_id_name <- 
  inner_join(G_data, name_links, by = 'schname')

final_df <- inner_join(g_trends_to_id_name, scorecard, by = c('unitid' = 'UNITID'))

# Creating one cost variable
final_df <- final_df %>% 
  mutate(
    cost = case_when(CONTROL == 1 ~ `NPT4_PUB-AVERAGE-ANNUAL-COST`,
                     CONTROL == 2 ~ NPT4_PRIV,
                     CONTROL == 3 ~ NPT4_PRIV)
    )


```

When choosing which variables to keep for the analysis I have looked at the Scorecard web site and the way data is presented there and tried to figure out what might influence someone's decision to investigate a particular college further (aka interest as measured by Google searches). The information that is prominently featured is (both when looking at a particular college, as well as Fields of study):

- graduation rate 

- average annual cost, 

- median earnings, 

- whether the school is private, public, 

- small or large, 

- in a city or not.
 
For this preliminary research, my assumption is that a ratio of average cost to median earnings and high graduation rate (potentially with looking at acceptance rate and SAT requirements) would influence someone's decision to further look into a particular college.

None of these variables are back doors: none of them would influence both earnings and interest in a particular college. 
What could be the back doors?
 - Reputation of the college - graduates of prestigious colleges are more likely to get higher earning jobs and these colleges are more likely to attract more interest from the prospective students. Proxy variables for this could be high graduation rate and low acceptance rate.

We have 1 snapshot in time of the college variables, but we can reasonably assume that none of them would change dramatically over a relatively short period of time of several years we are looking at. 

I will include these variables in the final data frame for ease of future analysis. We could potentially add fixed effects for college, but it is likely to introduce collinearity (I have tested it and the model does have collinearity). 

For dividing colleges into high and low earning ones I have decided to divide them by quartiles, as our income data is median, not mean. It will allow me to still have a decent sized sample for each group and will help me focus on a slightly wider range of schools making the groups more representative.

```{r echo = TRUE, error = FALSE, message=FALSE, warning=FALSE}

# preparing the final DF for analysis 

#select variables that we need and filter out non-predominantly BA colleges as well as closed colleges and creating Time variable for before and after scorecard 

finally_final_df <- final_df %>% 
  select('schname', 'month', 'st_index', 'cost', 'PREDDEG', 'SAT_AVG', 'CURROPER','md_earn_wne_p10-REPORTED-EARNINGS', 'C150_4_POOLED_SUPP-REPORTED-GRAD-RATE') %>% 
  filter(PREDDEG == 3, CURROPER == 1, `md_earn_wne_p10-REPORTED-EARNINGS` != 'PrivacySupressed') %>% 
  mutate(time = case_when(month >= as.Date("2015-09-01") ~ 1, month < as.Date("2015-09-01") ~ 0))


#Converting character to numeric to create NAs so that we can clean them out
finally_final_df <- finally_final_df %>% 
  mutate_if(is.character, as.numeric)

# final summarized df
G_summary <- finally_final_df %>% 
  group_by(schname, month) %>% 
  summarize(
    mean_index = mean(st_index),
    annual_cost = unique(cost),
    SAT = unique(SAT_AVG),
    grad_rate = unique(`C150_4_POOLED_SUPP-REPORTED-GRAD-RATE`),
    median_earnings = unique(`md_earn_wne_p10-REPORTED-EARNINGS`),
    time = unique(time)
    )
G_summary <- na.omit(G_summary)

#Looking at how median income is distributed between quartiles 
quantile(G_summary$median_earnings)
    #0%    25%    50%    75%   100% 
 #17600  36600  41400  47300 116400 

#Creating variable for groups by graduate earnings
G_summary <- G_summary %>% mutate(hi_low = case_when(median_earnings <= 36600 ~ 1, median_earnings < 41400 ~2, median_earnings < 47300 ~ 3, median_earnings > 47300 ~ 4)) %>% 
  filter(hi_low != 2 & hi_low != 3)

G_summary$hi_low <- factor(G_summary$hi_low)
G_summary$time <- factor(G_summary$time)

  
```


```{r echo = TRUE, error = FALSE, message=FALSE, warning=FALSE}

#Graphical analysis 

low_vs_high <- G_summary %>% 
  group_by(hi_low, month) %>% 
  summarize(index = mean(mean_index))%>% 
  ggplot(aes(x = month, y = index, colour = factor(hi_low, labels = c('Low median earnings', 'High median earnings')))) + 
  geom_line(aes(group = hi_low))+
  geom_vline(aes(xintercept = as.Date('2015-09-01')), color = 'blue') +
  theme_classic() +
  theme(axis.title.x = element_blank()) +
  labs(y = "Mean Google search index", 
       color = element_blank(),
       title = "Google search trends over time for colleges with high and low median earnings") 


low_vs_high


```

We can see a clear downward trend over the years

We can see a clear seasonality of searches

The gap between searches for colleges with low-income graduates vs colleges with high income graduates seems pretty small but increases slightly during peak times


```{r echo = TRUE, error = FALSE, message=FALSE, warning=FALSE}
#comparing the means
comp_means <- G_summary %>% 
  group_by(hi_low, time) %>% 
  summarize(mean(mean_index))
comp_means

# looking if the decrease over time we see on the graph is significant (for all colleges)
G_summary_full <- finally_final_df %>% 
  group_by(schname, month) %>% 
  summarize(
    mean_index = mean(st_index),
    median_earnings = unique(`md_earn_wne_p10-REPORTED-EARNINGS`),
    time = unique(time)
    )
G_summary_full <- na.omit(G_summary_full)


time_trends <- feols(mean_index ~ month, data = G_summary_full)
summary(time_trends)


#model 1
model1 <- feols(mean_index ~ time * hi_low, data = G_summary, vcov = 'hetero')

# Adding fixed effects for seasonality
G_seasonal <- mutate(G_summary, m_o_y = month(month))
model_seasonal <- feols(mean_index ~ time * hi_low | m_o_y, data = G_seasonal, vcov = 'hetero')

etable(model1, model_seasonal)

```
I am testing a linear model regressing Mean index on binary variable for time (before and after the introduction of the Scorecard) with an interaction term for high and low earning schools to see if the effect of the Scorecard on Index was different for these two groups.

After the introduction of the scorecard interest to high earning schools dropped by 3.2% more than for low earning schools and this difference is statistically significant at a 95% level. For both high and low earning colleges the introduction of a Scorecard lead to reduced interest and this effect was bigger for high earning schools. 

We see the same (and more statistically significant) effect in a model with added fixed effects for month of the year (since we saw on the graph that the search trends are highly seasonal)

So to answer our research question, yes, the model supports that interest towards high-earning colleges has shifted relative to low-earning colleges. But may be not how we initially expected - intuitively one might expect the interest to shift up rather than down. 

If we look at the broader picture and think about what might be happening here:

- The Google search trend for colleges was negative (see graph in the previous section) - we might be seeing some effects of this in our model. 

- ScoreCard is providing a lot of information so that people don't have to Google it, so they might use Google less for college research.

- ScoreCard provides direct links to colleges' web sites - that in turn might lead to people just clicking through to the college's web site directly, resulting in fewer Google searches.

- We don't know how was Scorecard promoted after it was launched. If no one knew about it for a while, and the growth was slow and organic, we might not have enough data for analysis. To check this theory, we can look at Scorecard's visitors' numbers for the same period.
  
 